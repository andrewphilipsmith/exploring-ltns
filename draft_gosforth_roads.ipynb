{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuERv-o6VAxA"
   },
   "source": [
    "# What's all this about?\n",
    "\n",
    "There are few aspects of modern life that have not been affected by Covid-19. Like many city's, Newcastle City Council introduced a number of changes with the intention of supporting social distancing. Amogst these were a number to the roads in Gosforth (where I live), closing certain roads to through motor traffic, establishing \"Low Traffic Neighbourhoods\" (LTNs).\n",
    "\n",
    "A local campaining group \"Space for Gosforth\", wrote [this blog article](http://spaceforgosforth.com/bollards) which pointed out that these measures are not new and there are already a number of local areas which are closed to through motor traffic, even if they weren't called LTNs origionally.\n",
    "\n",
    "These changes have their supporters and their detractors. One common accusation is that LTNs favour better off households, leaving poorer households to suffer higher, displaced traffic with the assicated collision and air polution risk. Is there any evidence to support this accusation related to the changes in Gosforth? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtfGXiIxTNCt"
   },
   "source": [
    "# Area of Interest\n",
    "\n",
    "Those who familar with Newcastle will know Gosforth includes some of the more affulent areas of the city. However this is not uniform and not everywhere in or around Gosforth is so well healed. \n",
    "\n",
    "\n",
    "The Area of Interest in somewhat arbitary, but for now I have limited myself to to the council wards which include the LTNs in the Space for Gosforth blog. Which is roughly the area bounded by, the Town Moor to the south, the A1 to the West, the Race Course to the north and boundary with North Tyneside to the north-east and cuts through parts of Heaton and Jesmond to the south-east. With the exception of the south-east, this generally conicides with some existing boundaries in the geography of the city.\n",
    "\n",
    "Since we're interested in the relationship of the relative wealth of neibourhoods which are designed to be \"low-traffic\" or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are we looking at this?\n",
    "\n",
    "There is a crude assumption in this analysis:\n",
    "* That any _benefits_ of LTNs occur for those households that are included in them. \n",
    "* That any _disadvantages_ of LTNs occur outside them.\n",
    "* \n",
    "The truth is, no doubt, a little more complex. Others have debated this at length elsewhere.\n",
    "\n",
    "The core question here is what is the range of wealth of the different households that are:\n",
    "a) in the newly established LTNs?\n",
    "b) in the previously established LTNs?\n",
    "c) not in any LTNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LhJeMxOfW2g"
   },
   "source": [
    "# How do we get this information?\n",
    "\n",
    "This is \n",
    "\n",
    "## Who lives inside and outside of the LTNs?\n",
    "\n",
    "The full information about who lives where and their releitive wealth is not available to a random punter who want to do their own analysis. From privacy point of view, this is a good thing.\n",
    "\n",
    "In the absence of data on where individuals live, a useful substitue can be the number of households. \n",
    "\n",
    "One of the best \n",
    "\n",
    "Just interested in residential properties.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What are the boundaries of the LTNs\n",
    "\n",
    "It is surprisingly hard to find a \n",
    "\n",
    "\n",
    "## How do we assess releitive wealth?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ6GOTNUDPsJ"
   },
   "source": [
    "# Setup some useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdgzmQwIDBm_",
    "outputId": "4063e27f-0025-4e9a-c3ed-25fd92422539"
   },
   "outputs": [],
   "source": [
    "!pip install icecream\n",
    "!pip install requests\n",
    "!pip install ipyleaflet\n",
    "!pip install matplotlib\n",
    "!pip install descartes\n",
    "# !pip install osmium\n",
    "!pip install overpy\n",
    "\n",
    "import shapely\n",
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import requests\n",
    "from icecream import ic\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "from osgeo import ogr\n",
    "import geopandas\n",
    "import matplotlib\n",
    "from ipyleaflet import Map, basemaps, GeoData, LayersControl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-t5o1MZ-c5n"
   },
   "source": [
    "# Some helpful functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHAiQkiI_qO5",
    "outputId": "6c019291-c224-4ff3-c94f-2efbd8d716fe"
   },
   "outputs": [],
   "source": [
    "# osm_file = 'tyne-and-wear-latest.osm.pbf'\n",
    "# osm_download_url = 'https://download.geofabrik.de/europe/great-britain/england/tyne-and-wear-latest.osm.pbf'\n",
    "# osm_lastest_md5sum_url = 'https://download.geofabrik.de/europe/great-britain/england/tyne-and-wear-latest.osm.pbf.md5'\n",
    "\n",
    "# Well known indentifers for the two coordinate systems used here.\n",
    "# Unprojected, WGS1984, Long, Lat\n",
    "# Prjected, British National Grid\n",
    "crs_long_lat = \"EPSG:4326\"\n",
    "crs_bng = \"EPSG:27700\"\n",
    "\n",
    "\n",
    "def _get_md5sum_local_file(f_path):\n",
    "    hash = hashlib.md5()\n",
    "    try:\n",
    "        if os.path.isfile(f_path):\n",
    "            with open(f_path, \"rb\") as lf:\n",
    "                hash.update(lf.read())\n",
    "    finally:\n",
    "        return hash.hexdigest()\n",
    "\n",
    "\n",
    "def _get_lastest_md5sum(md5sum_url):\n",
    "    response = requests.get(md5sum_url)\n",
    "    # We are only interested in the 32 char string, not the file name\n",
    "    match = re.search(\"^([0-9a-f]{32})[ ]{2}.*$\", response.text)\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def _download_file(download_url, target_path):\n",
    "    if not Path.exists(Path(target_path).parent):\n",
    "        Path.mkdir(Path(target_path).parent, parents=True)\n",
    "\n",
    "    with open(target_path, mode=\"wb\") as fb:\n",
    "        response = requests.get(download_url)\n",
    "        ic(response)\n",
    "        fb.write(response.content)\n",
    "\n",
    "\n",
    "def download_latest_file(download_url, target_filename, md5sum_url=None):\n",
    "    target_path = f\"downloads/{target_filename}\"\n",
    "    if md5sum_url:\n",
    "        if _get_md5sum_local_file(target_path) != _get_lastest_md5sum(md5sum_url):\n",
    "            ic(\"Remote files is different to localfile, downloading update.\")\n",
    "            _download_file(download_url, target_path)\n",
    "    else:\n",
    "        if not os.path.exists(target_path):\n",
    "            ic(\"local file does not exist, downloading new file.\")\n",
    "            _download_file(download_url, target_path)\n",
    "\n",
    "\n",
    "# https://download.geofabrik.de/europe/great-britain/england/tyne-and-wear.poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OSM from Geofabrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_file = \"tyne-and-wear-latest.osm.pbf\"\n",
    "osm_download_url = \"https://download.geofabrik.de/europe/great-britain/england/tyne-and-wear-latest.osm.pbf\"\n",
    "osm_lastest_md5sum_url = \"https://download.geofabrik.de/europe/great-britain/england/tyne-and-wear-latest.osm.pbf.md5\"\n",
    "\n",
    "# download_latest_file(osm_download_url, osm_file, osm_lastest_md5sum_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the routing server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo {osm_lastest_md5sum_url} osm_lastest_md5sum_url\n",
    "ic.disable()\n",
    "\n",
    "car_port = 5001\n",
    "bike_port = 5002\n",
    "walk_port = 5003\n",
    "\n",
    "osrm_docker_params = {\n",
    "    \"driving\": (\"data/osm/car\", car_port, \"/opt/car.lua\"),\n",
    "    \"cycling\": (\"data/osm/bike\", bike_port, \"/opt/bicycle.lua\"),\n",
    "    \"walking\": (\"data/osm/walk\", walk_port, \"/opt/foot.lua\"),\n",
    "}\n",
    "\n",
    "_pwd = %pwd\n",
    "\n",
    "for verb, values in osrm_docker_params.items():\n",
    "    data_path, host_port, profile_path = values\n",
    "    ic(data_path, host_port, profile_path, verb)\n",
    "    fq_path = Path(_pwd).joinpath(data_path)\n",
    "    ic(fq_path)\n",
    "    if not Path.exists(fq_path):\n",
    "        Path.mkdir(fq_path, parents=True)\n",
    "\n",
    "    shutil.copyfile(f\"downloads/{osm_file}\", f\"{fq_path}/{osm_file}\")\n",
    "\n",
    "    # Prep the data\n",
    "    !docker run -t -v \"{fq_path}:/data\" osrm/osrm-backend osrm-extract -p {profile_path} /data/tyne-and-wear-latest.osm.pbf\n",
    "    !docker run -t -v \"{fq_path}:/data\" osrm/osrm-backend osrm-partition /data/tyne-and-wear-latest.osrm\n",
    "    !docker run -t -v \"{fq_path}:/data\" osrm/osrm-backend osrm-customize /data/tyne-and-wear-latest.osrm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download boundaries from Ordnance Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries_download_url = \"https://api.os.uk/downloads/v1/products/BoundaryLine/downloads?area=GB&format=GeoPackage&redirect\"\n",
    "boundaries_zip_file = \"bdline_gpkg_gb.zip\"\n",
    "boundaries_file = \"data/boundaries/data/bdline_gb.gpkg\"\n",
    "\n",
    "download_latest_file(boundaries_download_url, boundaries_zip_file)\n",
    "\n",
    "if not os.path.exists(boundaries_file):\n",
    "    shutil.unpack_archive(f\"downloads/{boundaries_zip_file}\", \"data/boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract just the wards we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querry the county wards we are interested in and create a new layer\n",
    "shapely.speedups.disable()\n",
    "ward_list = [\n",
    "    \"Kenton Ward\",\n",
    "    \"Parklands Ward\",\n",
    "    \"Dene & South Gosforth Ward\",\n",
    "    \"Fawdon & West Gosforth Ward\",\n",
    "    \"North Jesmond Ward\",\n",
    "    \"Gosforth Ward\",\n",
    "]\n",
    "\n",
    "# In British National Grid\n",
    "all_wards_bng = geopandas.read_file(\n",
    "    Path(_pwd).joinpath(boundaries_file), layer=\"district_borough_unitary_ward\"\n",
    ")\n",
    "# ic(all_wards_gdf.length)\n",
    "\n",
    "all_wards_bng.head()\n",
    "\n",
    "aoi_wards_bng = all_wards_bng[all_wards_bng[\"Name\"].isin(ward_list)]\n",
    "# ic(aoi_wards)\n",
    "aoi_wards_bng = aoi_wards_bng[\n",
    "    aoi_wards_bng[\"File_Name\"].isin([\"NEWCASTLE_UPON_TYNE_DISTRICT_(B)\"])\n",
    "]\n",
    "# ic(aoi_wards)\n",
    "\n",
    "# In Lat/Long\n",
    "aoi_wards_ll = aoi_wards_bng.to_crs(crs=\"EPSG:4326\")\n",
    "\n",
    "# driver = ogr.GetDriverByName(\"GPKG\")\n",
    "# dataSource = driver.Open(all_boundaries, 0)\n",
    "# layer = dataSource.GetLayer('district_borough_unitary_ward')\n",
    "# ic(layer.GetFeatureCount())\n",
    "\n",
    "# # layer.SetAttributeFilter(ic(\"Name ='{}'\".format(\"' or Name ='\".join(ward_list))))\n",
    "# layer.SetAttributeFilter(ic(\"Name in ('{}')\".format(\"', '\".join(ward_list))))\n",
    "\n",
    "# ic(layer.GetFeatureCount())\n",
    "# for feature in layer:\n",
    "#     print(feature.GetField(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(_pwd)\n",
    "aoi_wards_bng.plot()\n",
    "aoi_wards_ll.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OSM building data (old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Download OSM streets from Overpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.enable()\n",
    "\n",
    "overpass_query = \"\"\"[timeout:25]\n",
    "[out:xml]\n",
    ";\n",
    "(\n",
    "  way\n",
    "    [\"highway\"]\n",
    "    (54.98785,-1.66885,55.04157,-1.57675);\n",
    "  relation\n",
    "    [\"highway\"]\n",
    "    (54.98785,-1.66885,55.04157,-1.57675);\n",
    ");\n",
    "(\n",
    "  node(w);\n",
    "  node(r);\n",
    ");\n",
    "out;\"\"\"\n",
    "\n",
    "import overpy\n",
    "import pandas as pd\n",
    "\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# fetch all ways and nodes\n",
    "result = api.query(overpass_query)\n",
    "ic(result)\n",
    "ic(len(result.nodes))\n",
    "ic(result.nodes[0])\n",
    "\n",
    "d = {\n",
    "    \"id\": [n.id for n in result.nodes],\n",
    "    \"lon\": [n.lon for n in result.nodes],\n",
    "    \"lat\": [n.lat for n in result.nodes],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "ic(df.head())\n",
    "ic(df.dtypes)\n",
    "all_street_nodes = geopandas.GeoDataFrame(\n",
    "    df.id, crs=\"EPSG:4326\", geometry=geopandas.points_from_xy(df.lon, df.lat)\n",
    ")\n",
    "ic(all_street_nodes.head())\n",
    "ic(all_street_nodes.dtypes)\n",
    "\n",
    "# all_street_nodes = geopandas.clip(all_street_nodes, aoi_wards_ll)\n",
    "all_street_nodes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download building from OpenStreetMap via Overpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAgU6HOEHg9w",
    "outputId": "5b9034b3-de7e-4597-aabf-ec0750fe944b"
   },
   "outputs": [],
   "source": [
    "# Download building from OpenStreetMap\n",
    "\n",
    "# Overpass query\n",
    "overpass_query = \"\"\"\n",
    "[out:xml] [timeout:25];\n",
    "(\n",
    "    way[\"building\"]( 54.98785,-1.66885,55.04157,-1.57675);\n",
    "    relation[\"building\"]( 54.98785,-1.66885,55.04157,-1.57675);\n",
    ");\n",
    "(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "# response = api.query(overpass_query)\n",
    "# ic(response)\n",
    "# ic(len(response.ways))\n",
    "# ic(len(response.relations))\n",
    "# ic(response.ways[0])\n",
    "\n",
    "# use this example https://gis.stackexchange.com/a/328608\n",
    "import shapely.geometry as geometry\n",
    "from shapely.ops import linemerge, unary_union, polygonize, voronoi_diagram\n",
    "import io\n",
    "import gdal\n",
    "\n",
    "# api = overpy.Overpass()\n",
    "\n",
    "# query = \"\"\" YOUR QUERY HERE \"\"\"\n",
    "# response = api.query(query)\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter?\"\n",
    "\n",
    "chunk_size = 1024\n",
    "parameters = {\"data\": overpass_query}\n",
    "r = requests.get(overpass_url, stream=True, params=parameters)\n",
    "# a = None\n",
    "# for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "#     if not a:\n",
    "#         a = chunk\n",
    "#     pass\n",
    "#     # fd.write(chunk)\n",
    "# ic(a)\n",
    "\n",
    "# geom_types = [\n",
    "#     \"points\",\n",
    "#     \"lines\",\n",
    "#     \"multilinestrings\",\n",
    "#     \"multipolygons\",\n",
    "#     \"other_relations\",\n",
    "# ]\n",
    "\n",
    "xml_path = \"data/osm/buildings/overpass_result.xml\"\n",
    "with open(xml_path, \"wb\") as xml_fb:\n",
    "    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "        xml_fb.write(chunk)\n",
    "\n",
    "out_file = \"data/osm/buildings/osm_buildings.gpkg\"\n",
    "\n",
    "in_ds = ogr.Open(xml_path)\n",
    "\n",
    "out_driver = ogr.GetDriverByName(\"GPKG\")\n",
    "gdal.SetConfigOption(\"OSM_USE_CUSTOM_INDEXING\", \"NO\")\n",
    "# Delete if previously exists\n",
    "if os.path.exists(out_file):\n",
    "    out_driver.DeleteDataSource(out_file)\n",
    "# Create file and layer\n",
    "out_ds = out_driver.CreateDataSource(out_file)\n",
    "in_lyr = in_ds.GetLayerByName(\"multipolygons\")\n",
    "out_lyr = out_ds.CopyLayer(\n",
    "    in_lyr, \"multipolygons\", options=[\"OVERWRITE=YES\", \"ENCODING=UTF-8\"]\n",
    ")\n",
    "out_lyr.SyncToDisk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_building_gdf = geopandas.read_file(out_file, layer=\"multipolygons\")\n",
    "ic(len(osm_building_gdf))\n",
    "ic(osm_building_gdf.columns)\n",
    "osm_building_gdf[[\"building\", \"name\"]].columns\n",
    "(osm_building_gdf[[\"building\", \"osm_way_id\"]]).groupby(\n",
    "    [\"building\"]\n",
    ").count().sort_values(\"osm_way_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"amenity\"=\"parking\"]( {{bbox}});\n",
    "    way[\"amenity\"=\"parking\"]( {{bbox}});\n",
    "    relation[\"amenity\"=\"parking\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\"\n",
    "\n",
    "school_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"amenity\"=\"kindergarten\"]( {{bbox}});\n",
    "    node[\"amenity\"=\"childcare\"]( {{bbox}});\n",
    "    node[\"amenity\"=\"school\"]( {{bbox}});\n",
    "    way[\"amenity\"=\"kindergarten\"]( {{bbox}});\n",
    "    way[\"amenity\"=\"childcare\"]( {{bbox}});\n",
    "    way[\"amenity\"=\"school\"]( {{bbox}});\n",
    "    relation[\"amenity\"=\"kindergarten\"]( {{bbox}});\n",
    "    relation[\"amenity\"=\"childcare\"]( {{bbox}});\n",
    "    relation[\"amenity\"=\"school\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\"\n",
    "\n",
    "healthcare_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"healthcare\"]( {{bbox}});\n",
    "    node[\"amenity\"=\"doctors\"]( {{bbox}});\n",
    "    way[\"healthcare\"]( {{bbox}});\n",
    "    way[\"amenity\"=\"doctors\"]( {{bbox}});\n",
    "    relation[\"healthcare\"]( {{bbox}});\n",
    "    relation[\"amenity\"=\"doctors\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\"\n",
    "\n",
    "offices_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"building\"=\"office\"]( {{bbox}});\n",
    "    node[\"office\"]( {{bbox}});\n",
    "    way[\"building\"=\"office\"]( {{bbox}});\n",
    "    way[\"office\"]( {{bbox}});\n",
    "    relation[\"building\"=\"office\"]( {{bbox}});\n",
    "    relation[\"office\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\"\n",
    "\n",
    "leisure_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"leisure\"]( {{bbox}});\n",
    "    way[\"leisure\"]( {{bbox}});\n",
    "    relation[\"leisure\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\"\n",
    "\n",
    "# Needs refinement\n",
    "outdoor_leisure_query = \"\"\"[out:xml] [timeout:25];\n",
    "(\n",
    "    node[\"natural\"]( {{bbox}});\n",
    "    node[\"leisure\"=\"park\"]( {{bbox}});\n",
    "    node[\"leisure\"=\"golf_course\"]( {{bbox}});\n",
    "    node[\"leisure\"=\"nature_reserve\"][\"landuse\"=\"allotments\"]( {{bbox}});\n",
    "    way[\"natural\"]( {{bbox}});\n",
    "    way[\"leisure\"=\"park\"]( {{bbox}});\n",
    "    way[\"leisure\"=\"golf_course\"]( {{bbox}});\n",
    "    way[\"leisure\"=\"nature_reserve\"][\"landuse\"=\"allotments\"]( {{bbox}});\n",
    "    relation[\"natural\"]( {{bbox}});\n",
    "    relation[\"leisure\"=\"park\"]( {{bbox}});\n",
    "    relation[\"leisure\"=\"golf_course\"]( {{bbox}});\n",
    "    relation[\"leisure\"=\"nature_reserve\"][\"landuse\"=\"allotments\"]( {{bbox}});\n",
    ");\n",
    "(._;>;);\n",
    "out body;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old code for parsing OSM overpy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {\n",
    "#     'building': [],\n",
    "#     'building_levels': [],\n",
    "#     'name': [],\n",
    "#     'geometry': []\n",
    "# }\n",
    "\n",
    "# 'building': 'apartments', 'building:levels': '15', 'name':\n",
    "\n",
    "# for ii_w,way in enumerate(response.ways):\n",
    "#     ls_coords = []\n",
    "#     # ic(ii_w)\n",
    "\n",
    "#     df['building'].append(way.tags.get('building', None))\n",
    "#     df['building_levels'].append(way.tags.get('building:levels', None))\n",
    "#     df['name'].append(way.tags.get('name', None))\n",
    "\n",
    "#     for node in way.nodes:\n",
    "#         ls_coords.append((node.lon,node.lat)) # create a list of node coordinates\n",
    "\n",
    "#     ls = geometry.LineString(ls_coords)\n",
    "#     df['geometry'].append(ls) # create a LineString from coords\n",
    "\n",
    "# merged = linemerge([*lss]) # merge LineStrings\n",
    "# borders = unary_union(merged) # linestrings to a MultiLineString\n",
    "# polygons = list(polygonize(borders))\n",
    "# ic(len(polygons))\n",
    "\n",
    "# osm_building_gdf = geopandas.geodataframe.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "# ic(len(osm_building_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(d)\n",
    "# ic(df.head())\n",
    "# ic(df.dtypes)\n",
    "# all_street_nodes = geopandas.GeoDataFrame(df.id, crs='EPSG:4326', geometry=geopandas.points_from_xy(df.lon, df.lat))\n",
    "# ic(all_street_nodes.head())\n",
    "# ic(all_street_nodes.dtypes)\n",
    "\n",
    "# all_street_nodes = geopandas.clip(all_street_nodes, aoi_wards_ll)\n",
    "# all_street_nodes.plot()\n",
    "\n",
    "\n",
    "# Borowing from here https://stackoverflow.com/a/45772571\n",
    "\n",
    "# import osmium as osm\n",
    "# import pandas as pd\n",
    "\n",
    "# class OSMHandler(osm.SimpleHandler):\n",
    "#     def __init__(self):\n",
    "#         osm.SimpleHandler.__init__(self)\n",
    "#         self.osm_data = []\n",
    "\n",
    "#     def tag_inventory(self, elem, elem_type):\n",
    "#         for tag in elem.tags:\n",
    "#             self.osm_data.append([elem_type,\n",
    "#                                    elem.id,\n",
    "#                                    elem.version,\n",
    "#                                    elem.visible,\n",
    "#                                    pd.Timestamp(elem.timestamp),\n",
    "#                                    elem.uid,\n",
    "#                                    elem.user,\n",
    "#                                    elem.changeset,\n",
    "#                                    len(elem.tags),\n",
    "#                                    tag.k,\n",
    "#                                    tag.v])\n",
    "\n",
    "#     def node(self, n):\n",
    "#         self.tag_inventory(n, \"node\")\n",
    "\n",
    "#     def way(self, w):\n",
    "#         self.tag_inventory(w, \"way\")\n",
    "\n",
    "#     def relation(self, r):\n",
    "#         self.tag_inventory(r, \"relation\")\n",
    "\n",
    "\n",
    "# osmhandler = OSMHandler()\n",
    "# # scan the input file and fills the handler list accordingly\n",
    "# osmhandler.apply_file(osm_file)\n",
    "\n",
    "# # transform the list into a pandas DataFrame\n",
    "# data_colnames = ['type', 'id', 'version', 'visible', 'ts', 'uid',\n",
    "#                  'user', 'chgset', 'ntags', 'tagkey', 'tagvalue']\n",
    "# df_osm = pd.DataFrame(osmhandler.osm_data, columns=data_colnames)\n",
    "# df_osm = tag_genome.sort_values(by=['type', 'id', 'ts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test osm_building_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # polygons[1:10]\n",
    "# w = response.ways[0]\n",
    "# ic(w)\n",
    "# ic(w.tags)\n",
    "# osm_building_gdf[0:1].plot()\n",
    "# osm_building_gdf.plot(column='building')\n",
    "# ic(osm_building_gdf[0:1])\n",
    "# # response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the LTN boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltns_path = \"data/ltns/ltns.geojson\"\n",
    "ltns_ll = geopandas.read_file(ltns_path)\n",
    "ltns_ll.plot(column=\"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imd_download_url = 'https://data.cdrc.ac.uk/system/files/Local_Data_Spaces/Local_Data_Spaces_E08000021.zip'\n",
    "# 'https://data-communities.opendata.arcgis.com/datasets/indices-of-multiple-deprivation-imd-2019-1/explore?filters=eyJMQURubSI6WyJOZXdjYXN0bGUgdXBvbiBUeW5lIl19&location=54.988609%2C-1.582592%2C12.00'\n",
    "\n",
    "# For info see https://data-communities.opendata.arcgis.com/datasets/communities::indices-of-multiple-deprivation-imd-2019-1/about\n",
    "imd_download_url = (\n",
    "    \"https://opendata.arcgis.com/datasets/4ad3e5a10872455eaa67ce4e663d0d01_0.geojson\"\n",
    ")\n",
    "imd_filename = \"imd.geojson\"\n",
    "imd_path = \"data/demography/imd.geojson\"\n",
    "\n",
    "download_latest_file(imd_download_url, imd_filename)\n",
    "\n",
    "if not os.path.exists(imd_path):\n",
    "    os.makedirs(\"data/demography\", exist_ok=True)\n",
    "    # .touch(exist_ok=True)\n",
    "    shutil.copyfile(f\"downloads/{imd_filename}\", imd_path)\n",
    "    # shutil.unpack_archive(f'downloads/{imd_zip_file}', 'data/demography')\n",
    "\n",
    "all_imd_ll = geopandas.read_file(imd_path)\n",
    "aoi_imd_ll = all_imd_ll[all_imd_ll[\"LADnm\"].isin([\"Newcastle upon Tyne\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_imd_ll.plot(column=\"IMDRank0\")\n",
    "aoi_imd_ll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download building info from Ordnance Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is updated every 6 weeks, so download a fresh copy\n",
    "# Scrape Version Date:2021-09 from https://osdatahub.os.uk/downloads/open/OpenUPRN\n",
    "\"https://api.os.uk/downloads/v1/products/OpenUPRN/downloads?area=GB&format=GeoPackage&redirect\"\n",
    "\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "osopenuprn_path = \"osopenuprn_202108_csv/osopenuprn_202107.csv\"\n",
    "aoi_osopenuprn_path = \"osopenuprn_202108_csv/aoi_uprn.shp\"\n",
    "\n",
    "df = pd.read_csv(osopenuprn_path)\n",
    "\n",
    "ic.enable()\n",
    "ic(len(df))\n",
    "ic(df.head())\n",
    "long_min, lat_min, long_max, lat_max = aoi_wards_ll.total_bounds\n",
    "\n",
    "# Initial crude clip for speed\n",
    "# clip north/south, then east/west\n",
    "df = df[(df.LATITUDE > lat_min) & (df.LATITUDE < lat_max)]\n",
    "ic(len(df))\n",
    "df = df[(df.LONGITUDE > long_min) & (df.LONGITUDE < long_max)]\n",
    "ic(len(df))\n",
    "\n",
    "aoi_uprn_ll_df = geopandas.GeoDataFrame(\n",
    "    df, crs=\"EPSG:4326\", geometry=geopandas.points_from_xy(df.LONGITUDE, df.LATITUDE)\n",
    ")\n",
    "aoi_uprn_df = geopandas.GeoDataFrame(\n",
    "    df,\n",
    "    crs=\"EPSG:27700\",\n",
    "    geometry=geopandas.points_from_xy(df.X_COORDINATE, df.Y_COORDINATE),\n",
    ")\n",
    "# ic(len(aoi_uprn_bng_df))\n",
    "# don't clip\n",
    "# aoi_uprn_df = geopandas.clip(aoi_uprn_bng_df, aoi_wards_bng)\n",
    "ic(len(aoi_uprn_df))\n",
    "aoi_uprn_df.head()\n",
    "\n",
    "aoi_uprn_df.to_file(aoi_osopenuprn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.enable()\n",
    "from time import sleep\n",
    "\n",
    "from_nodeid = 59642912  # 55.0079437, -1.6151497\n",
    "to_nodeid = 7019399129  # 55.0042099, -1.6202478\n",
    "\n",
    "ic(len(all_street_nodes))\n",
    "\n",
    "ic(all_street_nodes.index)\n",
    "\n",
    "\n",
    "# ic(all_street_nodes.id.isin([from_nodeid]))\n",
    "\n",
    "start_node = all_street_nodes[all_street_nodes.id.isin([from_nodeid])].iloc[0]\n",
    "end_node = all_street_nodes[all_street_nodes.id.isin([to_nodeid])].iloc[0]\n",
    "\n",
    "route_osrm_params = {\n",
    "    \"alternatives\": \"false\",\n",
    "    \"steps\": \"false\",\n",
    "    \"annotations\": \"nodes\",\n",
    "    \"geometries\": \"polyline\",\n",
    "    \"overview\": \"false\",\n",
    "    \"continue_straight\": \"default\",\n",
    "}\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     'driving': ('data/osm/car', car_port, '/opt/car.lua'),\n",
    "#     'cycling': ('data/osm/bike', bike_port, '/opt/bicycle.lua'),\n",
    "#     'walking': ('data/osm/walk', walk_port, '/opt/foot.lua')\n",
    "# }\n",
    "\n",
    "three_test_results = {}\n",
    "\n",
    "for verb, values in osrm_docker_params.items():\n",
    "    data_path, host_port, profile_path = values\n",
    "    ic(verb, data_path, host_port, profile_path)\n",
    "    host_port = 5000\n",
    "    fq_path = Path(_pwd).joinpath(data_path)\n",
    "    # Run the server\n",
    "    container_guid = !docker run -d -t -i -p {host_port}:5000 -v \"{fq_path}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/tyne-and-wear-latest.osrm\n",
    "    ic(container_guid)\n",
    "    sleep(5)\n",
    "    # 'http://127.0.0.1:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true'\n",
    "    base_osrm_url = f\"http://127.0.0.1:{host_port}/route/v1/{verb}/\"\n",
    "    route_osrm_url = f\"{base_osrm_url}{start_node.geometry.x},{start_node.geometry.y};{end_node.geometry.x},{end_node.geometry.y}\"\n",
    "    ic(route_osrm_url)\n",
    "\n",
    "    response = requests.get(route_osrm_url, route_osrm_params)\n",
    "    # ic(response.content)\n",
    "\n",
    "    route = json.loads(response.content)\n",
    "    three_test_results[verb] = route\n",
    "\n",
    "    route_nodeids = []\n",
    "    legs = route[\"routes\"][0][\"legs\"]\n",
    "    for l in legs:\n",
    "        # ic(l['annotation']['nodes'])\n",
    "        route_nodeids.extend(l[\"annotation\"][\"nodes\"])\n",
    "\n",
    "    ic(len(route_nodeids))\n",
    "    # ['legs']\n",
    "    route_node_df = all_street_nodes[all_street_nodes.id.isin(route_nodeids)]\n",
    "    ic(route_node_df.head())\n",
    "    route_node_df.plot()\n",
    "\n",
    "    !docker stop {container_guid[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verb in [\"driving\", \"cycling\", \"walking\"]:\n",
    "    details = three_test_results[verb][\"routes\"][0]\n",
    "    ic(\n",
    "        verb,\n",
    "        details[\"distance\"],\n",
    "        details[\"duration\"],\n",
    "        details[\"weight_name\"],\n",
    "        details[\"weight\"],\n",
    "    )\n",
    "\n",
    "# three_test_results[verb]['routes'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of _potential_ route start and end points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_routes = 10000\n",
    "min_seperation = 10  # meters\n",
    "max_seperation = 150  # meters\n",
    "ic.enable()\n",
    "route_list_cols = {\n",
    "    \"route_id\": [],\n",
    "    \"start_long\": [],\n",
    "    \"start_lat\": [],\n",
    "    \"start_node_id\": [],\n",
    "    \"end_long\": [],\n",
    "    \"end_lat\": [],\n",
    "    \"end_node_id\": [],\n",
    "    \"straight_line_dist\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def point_sampler(sample_size=number_of_routes):\n",
    "    sample_points = aoi_uprn_df.sample(2 * sample_size)\n",
    "    return sample_points.itertuples()\n",
    "\n",
    "\n",
    "def get_sample_point(point_iter):\n",
    "    try:\n",
    "        next_point = point_iter.__next__()\n",
    "        return next_point, point_iter\n",
    "    except StopIteration:\n",
    "        # ic('extending sampler')\n",
    "        point_iter = point_sampler(int(number_of_routes))\n",
    "        return get_sample_point(point_iter)\n",
    "\n",
    "\n",
    "def append_points(route_list_cols, start_point, end_point):\n",
    "    # route_list_cols['start_long'].append(start_point.geometry.x)\n",
    "    # route_list_cols['start_lat'].append(start_point.geometry.y)\n",
    "    route_list_cols[\"start_long\"].append(start_point.LONGITUDE)\n",
    "    route_list_cols[\"start_lat\"].append(start_point.LATITUDE)\n",
    "    route_list_cols[\"start_node_id\"].append(None)\n",
    "\n",
    "    # route_list_cols['end_long'].append(end_point.geometry.x)\n",
    "    # route_list_cols['end_lat'].append(end_point.geometry.y)\n",
    "    route_list_cols[\"end_long\"].append(end_point.LONGITUDE)\n",
    "    route_list_cols[\"end_lat\"].append(end_point.LATITUDE)\n",
    "    route_list_cols[\"end_node_id\"].append(None)\n",
    "    route_list_cols[\"straight_line_dist\"].append(dist)\n",
    "    return route_list_cols\n",
    "\n",
    "\n",
    "point_iter = point_sampler(number_of_routes)\n",
    "\n",
    "for idx in range(0, 2 * number_of_routes, 2):\n",
    "    route_list_cols[\"route_id\"].append(idx)\n",
    "    #\n",
    "    start_point, point_iter = get_sample_point(point_iter)\n",
    "    dist = 0\n",
    "    try_pair_count = 0\n",
    "\n",
    "    while not ((dist > min_seperation) and (dist < max_seperation)):\n",
    "        if try_pair_count == 10:\n",
    "            start_point, point_iter = get_sample_point(point_iter)\n",
    "            dist = 0\n",
    "            try_pair_count = 0\n",
    "\n",
    "        try_pair_count = try_pair_count + 1\n",
    "        end_point, point_iter = get_sample_point(point_iter)\n",
    "        dist = start_point.geometry.distance(end_point.geometry)\n",
    "        # ic(end_point)\n",
    "        # ic(idx, dist)\n",
    "\n",
    "    route_list_cols = append_points(route_list_cols, start_point, end_point)\n",
    "    # Add the reverse route\n",
    "    route_list_cols[\"route_id\"].append(idx + 1)\n",
    "    route_list_cols = append_points(route_list_cols, end_point, start_point)\n",
    "\n",
    "# for key, values in route_list_cols.items():\n",
    "#     ic(key, len(values))\n",
    "\n",
    "route_list_df = pd.DataFrame(route_list_cols)\n",
    "route_list_df.to_csv(\"results/route_list.csv\", index=False)\n",
    "# ic(route_list_df.head())\n",
    "# while sample_points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.enable()\n",
    "# ic(route_list_gdf.head())\n",
    "\n",
    "# route_list_gdf['geometry'] =\n",
    "#\n",
    "# ic(route_list_gdf['start_long', 'start_lat', 'end_long', 'end_lat'])\n",
    "\n",
    "# [geometry.LineString([(r.start_long, r.start_lat),(r.end_long, r.end_lat)]) for r in route_list_gdf]\n",
    "\n",
    "# DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwargs)[source]\n",
    "\n",
    "# route_list_df['geometry'] = None\n",
    "\n",
    "\n",
    "# There must be a better, set-wise way of achiving this, but I cannot get any of them to work\n",
    "for i in route_list_df.index.array:\n",
    "    r = route_list_df.loc[i]\n",
    "    route_list_df.loc[i, \"geometry\"] = geometry.LineString(\n",
    "        [(r.start_long, r.start_lat), (r.end_long, r.end_lat)]\n",
    "    )\n",
    "\n",
    "route_list_gdf = geopandas.GeoDataFrame(\n",
    "    route_list_df, geometry=\"geometry\", crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "\n",
    "####\n",
    "# Various failed attempts to create lines in a set-wise method below\n",
    "####\n",
    "\n",
    "# def create_line(r):\n",
    "#     # ic(r)\n",
    "#     return geometry.LineString([(r[0],r[1]),(r[2],r[3])])\n",
    "\n",
    "\n",
    "# temp_line_list = route_list_gdf[['start_long', 'start_lat', 'end_long', 'end_lat']].apply(create_line, axis=1, raw=True)\n",
    "# temp_line_list.head()\n",
    "# route_list_gdf['geometry'] = temp_line_list\n",
    "\n",
    "# route_list_gdf['geometry'] = ([\n",
    "#         (route_list_gdf['start_long'], route_list_gdf['start_lat']),\n",
    "#         (route_list_gdf['end_long'], route_list_gdf['end_lat'])\n",
    "#     ])\n",
    "\n",
    "# ic(route_list_gdf.head())\n",
    "# route_list_gdf['geometry'].apply(lambda r: geometry.LineString([(r.start_long, r.start_lat),(r.end_long, r.end_lat)]))\n",
    "# axis=1,  result_type='expand',\n",
    "# route_list_gdf['geometry'] = geometry.LineString([\n",
    "#     (route_list_gdf['start_long'], route_list_gdf['start_lat']),\n",
    "#     (route_list_gdf['end_long'], route_list_gdf['end_lat'])\n",
    "#     ])\n",
    "\n",
    "# route_list_gdf.set_geometry = 'geometry'\n",
    "\n",
    "# route_list_gdf = geopandas.GeoDataFrame(route_list_df, geometry=geometry.LineString([\n",
    "#     (route_list_df.start_long, route_list_df.start_lat),\n",
    "#     (route_list_df.end_long, route_list_df.end_lat)]),\n",
    "#     crs='EPSG:4326')\n",
    "\n",
    "\n",
    "route_list_gdf.to_file(\"results/route_list.shp\")\n",
    "route_list_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snap addresses to road network\n",
    "\n",
    "Note that this ignores the cases where there are different enterences for different transport types (eg parking at large supermarkets), but prevents a more common problem of addresses snapping to different parts of the network, resulting in routes that are not really comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicats from aoi_uprn_df\n",
    "# in some cases (eg a block of flats, or PO boxes) there many be multiple points, representing multiple postal delivery points, all with idenical geographic locations.\n",
    "# ic(aoi_uprn_df.head())\n",
    "unique_address_points_by_ll = aoi_uprn_df.groupby(\n",
    "    [\"LATITUDE\", \"LONGITUDE\"], as_index=False\n",
    ").count()\n",
    "unique_address_points_by_ll.rename(columns={\"UPRN\": \"my_UPRN\"}, inplace=True)\n",
    "unique_address_points_by_ll.set_index(\"my_UPRN\")\n",
    "ic(unique_address_points_by_ll.head())\n",
    "# unique_address_points_by_ll.to_file('results/unique_address_points_by_ll.shp')\n",
    "unique_address_points_by_bng = aoi_uprn_df.groupby(\n",
    "    [\"X_COORDINATE\", \"Y_COORDINATE\"], as_index=False\n",
    ").count()\n",
    "unique_address_points_by_bng.rename(columns={\"UPRN\": \"my_UPRN\"}, inplace=True)\n",
    "unique_address_points_by_bng.set_index(\"my_UPRN\")\n",
    "ic(unique_address_points_by_bng.head())\n",
    "ic(aoi_uprn_df.head())\n",
    "# unique_address_points_by_bng.to_file('results/unique_address_points_by_bng.shp')\n",
    "\n",
    "# ic(len(unique_address_points))\n",
    "\n",
    "# # using XY\n",
    "# # ic| len(aoi_uprn_df): 53992\n",
    "# # ic| len(unique_address_points): 10737\n",
    "\n",
    "# # using Lat,Long\n",
    "# # ic| len(aoi_uprn_df): 53992\n",
    "# # ic| len(unique_address_points): 42916\n",
    "\n",
    "# unique_address_points.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shapely==1.8rc1\n",
    "# !pip show shapely\n",
    "# from shapely.ops import voronoi_diagram\n",
    "# # address_polys_gdf = aoi_uprn_df.copy()\n",
    "# # address_polys_gdf['geometry']\n",
    "# address_polys = voronoi_diagram(aoi_uprn_df['geometry'])\n",
    "# ic(len(address_polys))\n",
    "\n",
    "aoi_uprn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do snapping using routing server\n",
    "\n",
    "data_path, host_port, profile_path = osrm_docker_params[\"driving\"]\n",
    "ic(data_path, host_port, profile_path)\n",
    "\n",
    "host_port = 5000\n",
    "fq_path = Path(_pwd).joinpath(data_path)\n",
    "\n",
    "container_guid = !docker run -d -t -i -p {host_port}:5000 -v \"{fq_path}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/tyne-and-wear-latest.osrm\n",
    "ic(container_guid)\n",
    "sleep(5)\n",
    "\n",
    "# from_nodeid = 59642912  # 55.0079437, -1.6151497\n",
    "# to_nodeid = 7019399129  # 55.0042099, -1.6202478\n",
    "\n",
    "# 55.008281099999998\n",
    "# -1.615038900000000\n",
    "\n",
    "# \t\t\t\t-1.615252,\n",
    "# \t\t\t\t55.008258\n",
    "\n",
    "\n",
    "# # futher north up Hartley terrace\n",
    "# 55.008973699999999\n",
    "# -1.615266800000000\n",
    "\n",
    "\n",
    "# 'http://127.0.0.1:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true'\n",
    "# http://{server}/nearest/v1/{profile}/{coordinates}.json?number={number}\n",
    "\n",
    "# 'http://127.0.0.1:5000/nearest/v1/driving/-1.615038900000000,55.008281099999998?number=1&generate_hints=false'\n",
    "# 'http://127.0.0.1:5000/nearest/v1/driving/-1.615266800000000,55.008973699999999?number=1&generate_hints=false'\n",
    "\n",
    "# 'http://127.0.0.1:5000/nearest/v1/driving/-1.6123302,55.0102618?number=1&generate_hints=false'\n",
    "\n",
    "# 55.0102618,-1.6123302\n",
    "\n",
    "\n",
    "base_snap_osrm_url = f\"http://127.0.0.1:{host_port}/nearest/v1/driving/\"\n",
    "\n",
    "\n",
    "nearest_osrm_params = {\"number\": 1, \"generate_hints\": \"false\"}\n",
    "\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "address_street_node_cols = {\"UPRN\": [], \"street_node_id\": [], \"geometry\": []}\n",
    "\n",
    "########\n",
    "# Using the routing server here, gaurentees that we are snapping to somewhere on the driving road network (not footpaths etc)\n",
    "########\n",
    "mybreak = 0\n",
    "for address in aoi_uprn_df.itertuples():\n",
    "    mybreak = mybreak + 1\n",
    "    # if mybreak>4000:\n",
    "    #     break\n",
    "\n",
    "    request_count = request_count + 1\n",
    "    if request_count % 400 == 0:\n",
    "        sleep(1)\n",
    "\n",
    "    # get a list of the nestest street nodes\n",
    "\n",
    "    nearest_osrm_url = f\"{base_snap_osrm_url}{address.LONGITUDE},{address.LATITUDE}\"\n",
    "    # ic(nearest_osrm_url)\n",
    "\n",
    "    response = requests.get(nearest_osrm_url, nearest_osrm_params)\n",
    "    # ic(response.content)\n",
    "\n",
    "    nearest = json.loads(response.content)\n",
    "    nearby_nodes_ids = nearest[\"waypoints\"][0][\"nodes\"]\n",
    "    # ic(address.geometry.x)\n",
    "    # ic(address.geometry.y)\n",
    "\n",
    "    nearby_nodes_gdf = all_street_nodes[\n",
    "        all_street_nodes.id.isin(nearby_nodes_ids)\n",
    "    ].copy(deep=True)\n",
    "\n",
    "    # typically this happens if there is an address near teh edge of the study area, and the nearest\n",
    "    # matching street nodes are outside the study areas. This is literally an edge case that can be ignored.\n",
    "    if len(nearby_nodes_gdf) > 0:\n",
    "        address_street_node_cols[\"UPRN\"].append(address.UPRN)\n",
    "        # reproject for comparision with address data\n",
    "        nearby_nodes_gdf = nearby_nodes_gdf.to_crs(aoi_uprn_df.crs)\n",
    "        nearby_nodes_gdf[\"dist\"] = nearby_nodes_gdf[\"geometry\"].apply(\n",
    "            lambda x: address.geometry.distance(x)\n",
    "        )\n",
    "        # ic(nearby_nodes_gdf)\n",
    "        # get the nearest node\n",
    "        nearest_node = nearby_nodes_gdf.loc[nearby_nodes_gdf[\"dist\"].idxmin()]\n",
    "\n",
    "        # add the node ID\n",
    "        address_street_node_cols[\"street_node_id\"].append(nearest_node.id)\n",
    "        # create a line between the address and the street node\n",
    "        address_street_node_cols[\"geometry\"].append(\n",
    "            geometry.LineString([nearest_node.geometry, address.geometry])\n",
    "        )\n",
    "\n",
    "        # r = route_list_df.loc[i]\n",
    "        # route_list_df.loc[i,'geometry'] = geometry.LineString([(r.start_long, r.start_lat),(r.end_long, r.end_lat)])\n",
    "\n",
    "address_street_node_gdf = geopandas.GeoDataFrame(\n",
    "    address_street_node_cols, geometry=\"geometry\", crs=aoi_uprn_df.crs\n",
    ")\n",
    "address_street_node_gdf.to_file(\"results/address_street_node_pairs.shp\")\n",
    "# aoi_uprn_df\n",
    "\n",
    "# Now stop docker\n",
    "!docker stop {container_guid[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cardinal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_street_node_clipped_gdf = geopandas.clip(address_street_node_gdf, aoi_wards_bng)\n",
    "unique_street_node_ids = address_street_node_clipped_gdf[\"street_node_id\"].unique()\n",
    "ic(len(address_street_node_gdf))\n",
    "ic(len(address_street_node_clipped_gdf))\n",
    "ic(len(unique_street_node_ids))\n",
    "unique_street_node_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_street_nodes_gdf = all_street_nodes[all_street_nodes.id.isin(unique_street_node_ids)].copy(deep=True)\n",
    "# unique_street_nodes_gdf.head()\n",
    "# nearby_nodes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "# Create GDF of unique street nodes\n",
    "unique_street_nodes_gdf = all_street_nodes[all_street_nodes.id.isin(unique_street_node_ids)].copy(deep=True)\n",
    "# Make a copy of teh Long/Lat values for later use with the routing engine\n",
    "# Then do everything in BNG for for geo calculations\n",
    "unique_street_nodes_gdf['LONG'] = unique_street_nodes_gdf['geometry'].x\n",
    "unique_street_nodes_gdf['LAT'] = unique_street_nodes_gdf['geometry'].y\n",
    "unique_street_nodes_gdf = unique_street_nodes_gdf.to_crs(aoi_uprn_df.crs)\n",
    "\n",
    "data_path, host_port, profile_path = osrm_docker_params['driving']\n",
    "ic(data_path, host_port, profile_path)\n",
    "\n",
    "host_port = 5000\n",
    "fq_path = Path(_pwd).joinpath(data_path)\n",
    "\n",
    "container_guid = !docker run -d -t -i -p {host_port}:5000 -v \"{fq_path}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/tyne-and-wear-latest.osrm\n",
    "ic(container_guid)\n",
    "sleep(5)\n",
    "\n",
    "base_snap_osrm_url = f'http://127.0.0.1:{host_port}/nearest/v1/driving/'\n",
    "\n",
    "nearest_osrm_params = {\n",
    "    'number': 1,\n",
    "    'generate_hints': 'false'\n",
    "}\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "address_street_node_cols = {\n",
    "    'source_node_id': [],\n",
    "    'geometry': []\n",
    "}\n",
    "\n",
    "\n",
    "target_list_cols = {\n",
    "    'start_node_id': [],\n",
    "    'geometry': [],\n",
    "    'intented_offset_dist': [],\n",
    "    'intented_offset_bearing': []\n",
    "}\n",
    "\n",
    "intented_offsets = [50, 250, 1500]\n",
    "intented_offset_lambdas = []\n",
    "\n",
    "intented_offset_lambdas.append( ('n', 50, lambda x, y, z=None: (x+0, y+50)) )\n",
    "intented_offset_lambdas.append( ('e', 50, lambda x, y, z=None: (x+50, y+0)) )\n",
    "intented_offset_lambdas.append( ('s', 50, lambda x, y, z=None: (x+0, y-50)) )\n",
    "intented_offset_lambdas.append( ('w', 50, lambda x, y, z=None: (x-50, y+0)) )\n",
    "intented_offset_lambdas.append( ('n', 250, lambda x, y, z=None: (x+0, y+250)) )\n",
    "intented_offset_lambdas.append( ('e', 250, lambda x, y, z=None: (x+250, y+0)) )\n",
    "intented_offset_lambdas.append( ('s', 250, lambda x, y, z=None: (x+0, y-250)) )\n",
    "intented_offset_lambdas.append( ('w', 250, lambda x, y, z=None: (x-250, y+0)) )\n",
    "intented_offset_lambdas.append( ('n', 1500, lambda x, y, z=None: (x+0, y+1500)) )\n",
    "intented_offset_lambdas.append( ('e', 1500, lambda x, y, z=None: (x+1500, y+0)) )\n",
    "intented_offset_lambdas.append( ('s', 1500, lambda x, y, z=None: (x+0, y-1500)) )\n",
    "intented_offset_lambdas.append( ('w', 1500, lambda x, y, z=None: (x-1500, y+0)) )\n",
    "\n",
    "######## \n",
    "# Using the routing server here, gaurentees that we are snapping to somewhere on the driving road network (not footpaths etc)\n",
    "########\n",
    "\n",
    "# Create GDF of target points in BNG\n",
    "for street_node in unique_street_nodes_gdf.itertuples():\n",
    "    for intented_bearing, intented_dist, func in intented_offset_lambdas:\n",
    "        target_point = shapely.ops.transform(func, street_node.geometry)\n",
    "\n",
    "        target_list_cols['start_node_id'].append(street_node.id)\n",
    "        target_list_cols['geometry'].append(target_point)\n",
    "        target_list_cols['intented_offset_dist'].append(intented_dist)\n",
    "        target_list_cols['intented_offset_bearing'].append(intented_bearing)\n",
    "\n",
    "target_gdf = geopandas.GeoDataFrame(target_list_cols, geometry='geometry', crs=unique_street_nodes_gdf.crs)\n",
    "target_gdf = target_gdf.to_crs(crs='EPSG:4326')\n",
    "target_gdf.to_file('results/target_cardial_points.shp')\n",
    "\n",
    "ic(len(unique_street_nodes_gdf))\n",
    "ic(len(target_gdf))\n",
    "ic(len(intented_offset_lambdas))\n",
    "\n",
    "route_list_cols = {\n",
    "    'route_id': [],\n",
    "    'start_long': [],\n",
    "    'start_lat': [],\n",
    "    'start_node_id': [],\n",
    "    'end_long': [],\n",
    "    'end_lat': [],\n",
    "    'end_node_id': [],\n",
    "    'intented_offset_dist': [],\n",
    "    'intented_offset_bearing': [],\n",
    "    'geometry': []\n",
    "}\n",
    "\n",
    "mybreak = 0\n",
    "request_count = 0\n",
    "for target_point in target_gdf.itertuples():\n",
    "    # mybreak = mybreak +1\n",
    "    # if mybreak>20:\n",
    "    #     break\n",
    "\n",
    "    request_count = request_count + 1\n",
    "    if request_count % 400 == 0:\n",
    "        sleep(1)\n",
    "\n",
    "    # get a list of the nestest street nodes\n",
    "    nearest_osrm_url = f'{base_snap_osrm_url}{target_point.geometry.x},{target_point.geometry.y}'\n",
    "    response = requests.get(nearest_osrm_url, nearest_osrm_params)\n",
    "    nearest = json.loads(response.content)\n",
    "    nearby_nodes_ids = nearest['waypoints'][0]['nodes']\n",
    "\n",
    "    # ic(nearby_nodes_ids)\n",
    "\n",
    "    # both = [7736056868, 725625685]\n",
    "    # exclude = 725625685\n",
    "\n",
    "    # Prevent snapping back to the start point\n",
    "    nearby_nodes_ids = [n_id for n_id in nearby_nodes_ids if not n_id == target_point.start_node_id]\n",
    "\n",
    "    nearby_nodes_gdf = all_street_nodes[all_street_nodes.id.isin(nearby_nodes_ids)].copy(deep=True)\n",
    "\n",
    "    # typically this happens if there is an address near teh edge of the study area, and the nearest\n",
    "    # matching street nodes are outside the study areas. This is literally an edge case that can be ignored.\n",
    "    if len(nearby_nodes_gdf) > 0:\n",
    "        # reproject for comparision with address data\n",
    "        nearby_nodes_gdf = nearby_nodes_gdf.to_crs(aoi_uprn_df.crs)\n",
    "        nearby_nodes_gdf['dist'] = nearby_nodes_gdf['geometry'].apply(lambda x: target_point.geometry.distance(x))\n",
    "        # get the nearest node\n",
    "        nearby_nodes_gdf.to_crs(crs='EPSG:4326')\n",
    "        nearest_node = nearby_nodes_gdf.loc[nearby_nodes_gdf['dist'].idxmin()]\n",
    "\n",
    "        # this Should be a query\n",
    "        # start_node_ll = all_street_nodes[all_street_nodes.id.equals(target_point.start_node_id)]\n",
    "        start_node = unique_street_nodes_gdf[unique_street_nodes_gdf.id.isin([target_point.start_node_id])].iloc[0]\n",
    "\n",
    "\n",
    "        # add the node ID\n",
    "        route_list_cols['route_id'].append(request_count)\n",
    "        route_list_cols['start_long'].append(start_node.LONG)\n",
    "        route_list_cols['start_lat'].append(start_node.LAT)\n",
    "        route_list_cols['start_node_id'].append(target_point.start_node_id)\n",
    "        route_list_cols['end_long'].append(nearest_node.geometry.x)\n",
    "        route_list_cols['end_lat'].append(nearest_node.geometry.y)\n",
    "        route_list_cols['end_node_id'].append(nearest_node.id)\n",
    "        route_list_cols['intented_offset_dist'].append(target_point.intented_offset_dist)\n",
    "        route_list_cols['intented_offset_bearing'].append(target_point.intented_offset_bearing)\n",
    "        # real_dist = start_node_bng.geometry.distance(nearest_node.geometry)\n",
    "        # ic(target_point.intented_offset_dist, real_dist)\n",
    "        # route_list_cols['actual_line_dist'].append(real_dist)\n",
    "        # create a line between the start and actual end point\n",
    "        geom = geometry.LineString([\n",
    "            (start_node.geometry.x,start_node.geometry.y),\n",
    "            (nearest_node.geometry.x, nearest_node.geometry.y)\n",
    "        ])\n",
    "        route_list_cols['geometry'].append(geom)\n",
    "\n",
    "        # r = route_list_df.loc[i]\n",
    "        # route_list_df.loc[i,'geometry'] = geometry.LineString([(r.start_long, r.start_lat),(r.end_long, r.end_lat)])\n",
    "\n",
    "route_list_gdf = geopandas.GeoDataFrame(route_list_cols, geometry='geometry', crs=aoi_uprn_df.crs)\n",
    "# route_list_gdf.plot()\n",
    "route_list_gdf.to_file('results/route_list_carinal_pairs.shp')\n",
    "# aoi_uprn_df\n",
    "\n",
    "# Now stop docker\n",
    "!docker stop {container_guid[0]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_list_gdf.head(20)\n",
    "# ic(nearby_nodes_gdf)\n",
    "# ic(target_point)\n",
    "# ic(nearby_nodes_ids)\n",
    "\n",
    "# both = [7736056868, 725625685]\n",
    "# exclude = 725625685\n",
    "\n",
    "# a = [x for x in both if not x == exclude]\n",
    "# ic(a)\n",
    "# nearby_nodes_ids\n",
    "# ic(unique_street_nodes_gdf.head())\n",
    "# 11143086\n",
    "# start_node_ll = unique_street_nodes_gdf\n",
    "# a = unique_street_nodes_gdf[unique_street_nodes_gdf.id.isin([11143086])]\n",
    "# ic(type(a.iloc[0].geometry))\n",
    "# # ic(len(a.iloc[0].geometry))\n",
    "# ic(a.iloc[0])\n",
    "# ic(type(nearest_node.geometry.x))\n",
    "# ic(type(target_point.geometry.x))\n",
    "# ic(type(start_node.geometry.x))\n",
    "#  ic(nearest_node)\n",
    "# ic(len(address_street_node_gdf))\n",
    "\n",
    "# address_street_node_gdf['street_node_id'].unique()\n",
    "\n",
    "# ic(len(address_street_node_gdf['street_node_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \"D:\\andy\\space-for-gosforth\\results\\demo_route_pairs.shp\"\n",
    "route_list_gdf = geopandas.read_file(\"results/demo_route_pairs.shp\")\n",
    "route_list_cols = {\n",
    "    \"route_id\": [],\n",
    "    \"start_long\": [],\n",
    "    \"start_lat\": [],\n",
    "    \"start_node_id\": [],\n",
    "    \"end_long\": [],\n",
    "    \"end_lat\": [],\n",
    "    \"end_node_id\": [],\n",
    "    \"intented_offset_dist\": [],\n",
    "    \"intented_offset_bearing\": [],\n",
    "    \"geometry\": [],\n",
    "}\n",
    "\n",
    "ic(route_list_gdf.columns)\n",
    "route_list_gdf.columns = route_list_cols.keys()\n",
    "ic(route_list_gdf.columns)\n",
    "\n",
    "#  ['route_id', 'start_long', 'start_lat', 'start_node', 'end_long',\n",
    "#                                    'end_lat', 'end_node_id', 'intented_o', 'intented_1', 'geometry']\n",
    "ic(route_list_gdf.head())\n",
    "ic(all_street_nodes.head())\n",
    "####\n",
    "# Convert route_list_gdf back to lat/long\n",
    "route_list_gdf = route_list_gdf.to_crs(crs=\"EPSG:4326\")\n",
    "# a = route_list_gdf.head()\n",
    "\n",
    "# DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False,\n",
    "# right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)[source]\n",
    "\n",
    "\n",
    "########################\n",
    "########################\n",
    "########################\n",
    "#\n",
    "#\n",
    "#  DONT RE-RUN\n",
    "#\n",
    "#\n",
    "########################\n",
    "########################\n",
    "########################\n",
    "# route_list_gdf_old = route_list_gdf\n",
    "\n",
    "route_list_gdf = route_list_gdf.merge(\n",
    "    all_street_nodes,\n",
    "    how=\"inner\",\n",
    "    left_on=\"end_node_id\",\n",
    "    right_on=\"id\",\n",
    "    suffixes=(\"\", \"all\"),\n",
    "    copy=True,\n",
    ")\n",
    "route_list_gdf[\"end_long\"] = route_list_gdf.apply(\n",
    "    lambda row: row[\"geometryall\"].x, axis=1\n",
    ")\n",
    "route_list_gdf[\"end_lat\"] = route_list_gdf.apply(\n",
    "    lambda row: row[\"geometryall\"].y, axis=1\n",
    ")\n",
    "\n",
    "# .apply(lambda row: row['driving_count'] - row['cycling_count'], axis=1)\n",
    "\n",
    "# b = a.join(all_street_nodes, on='id', how='inner', lsuffix='route', rsuffix='all')\n",
    "# ic(a)\n",
    "# ic(b)\n",
    "ic(route_list_gdf.head())\n",
    "# ic(all_street_nodes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(route_list_gdf.head())\n",
    "ic(route_list_gdf.columns)\n",
    "route_list_gdf.drop(columns=[\"id\", \"geometryall\", \"idall\", \"geometryall\"])\n",
    "ic(route_list_gdf.columns)\n",
    "\n",
    "ic(len(route_list_gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk calculate routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ic.enable()\n",
    "route_osrm_params = {\n",
    "    'alternatives': 'false',\n",
    "    'steps': 'false',\n",
    "    'annotations': 'true',\n",
    "    'geometries': 'geojson',\n",
    "    'overview': 'full',\n",
    "    'continue_straight': 'default'\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "all_results_summary = {\n",
    "    'route_id': [],\n",
    "    'verb': [],\n",
    "    'distance': [],\n",
    "    'duration': [],\n",
    "    'weight_name': [],\n",
    "    'weight': [],\n",
    "    'geometry': []\n",
    "}\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     'driving': ('data/osm/car', car_port, '/opt/car.lua'),\n",
    "#     'cycling': ('data/osm/bike', bike_port, '/opt/bicycle.lua'),\n",
    "#     'walking': ('data/osm/walk', walk_port, '/opt/foot.lua')\n",
    "# }\n",
    "\n",
    "for verb, values in osrm_docker_params.items():\n",
    "    data_path, host_port, profile_path = values\n",
    "    ic(verb, data_path, host_port, profile_path)\n",
    "    host_port = 5000\n",
    "    fq_path = Path(_pwd).joinpath(data_path)\n",
    "    # Run the server\n",
    "    container_guid = !docker run -d -t -i -p {host_port}:5000 -v \"{fq_path}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/tyne-and-wear-latest.osrm\n",
    "    ic(container_guid)\n",
    "    sleep(5)\n",
    "    # 'http://127.0.0.1:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true'\n",
    "    base_osrm_url = f'http://127.0.0.1:{host_port}/route/v1/{verb}/'\n",
    "\n",
    "    result_list_cols = {\n",
    "        'route_id': [],\n",
    "        'node_id': [],\n",
    "    }\n",
    "\n",
    "    # ic.disable()\n",
    "    for route_pairs in route_list_gdf.itertuples():\n",
    "        # Pause every 100's route, to prevent overloading the routing server    \n",
    "        if route_pairs.route_id % 200 == 0:\n",
    "            sleep(1)\n",
    "\n",
    "        if route_pairs.route_id % 1000 == 0:\n",
    "            ic(route_pairs.route_id)\n",
    "\n",
    "        # Pandas(Index=0, route_id=0, start_long=-1.6400524, start_lat=54.9964598, start_node_id=None, end_long=-1.5980157, end_lat=55.0031938, end_node_id=None, straight_line_50=2791.5803767758507)\n",
    "        route_osrm_url = f'{base_osrm_url}{route_pairs.start_long},{route_pairs.start_lat};{route_pairs.end_long},{route_pairs.end_lat}'\n",
    "        # reverse the route\n",
    "        # route_osrm_url = f'{base_osrm_url}{route_pairs.end_long},{route_pairs.end_lat};{route_pairs.start_long},{route_pairs.start_lat}'\n",
    "        # ic(route_osrm_url)\n",
    "\n",
    "\n",
    "        # ic(route_pairs.route_id)\n",
    "        response = requests.get(route_osrm_url, route_osrm_params)\n",
    "        # ic(response.content)\n",
    "        route = json.loads(response.content)\n",
    "\n",
    "        if route['code'] != 'Ok':\n",
    "            all_results_summary['route_id'].append(route_pairs.route_id)\n",
    "            all_results_summary['verb'].append(verb)\n",
    "            all_results_summary['distance'].append(0)\n",
    "            all_results_summary['duration'].append(0)\n",
    "            all_results_summary['weight_name'].append('NoRoute')\n",
    "            all_results_summary['weight'].append(0)\n",
    "            all_results_summary['geometry'].append(None)\n",
    "        else:\n",
    "            details = route['routes'][0]\n",
    "            all_results_summary['route_id'].append(route_pairs.route_id)\n",
    "            all_results_summary['verb'].append(verb)\n",
    "            all_results_summary['distance'].append(details['distance'])\n",
    "            all_results_summary['duration'].append(details['duration'])\n",
    "            all_results_summary['weight_name'].append(details['weight_name'])\n",
    "            all_results_summary['weight'].append(details['weight'])\n",
    "\n",
    "            route_nodeids = []\n",
    "            legs = details['legs']\n",
    "            for l in legs:\n",
    "                # ic(l['annotation']['nodes'])\n",
    "                route_nodeids.extend(l['annotation']['nodes'])\n",
    "\n",
    "            # ic(route_pairs.route_id, len(route_nodeids), route_pairs.straight_line_dist)\n",
    "            # repeat the route id for all values\n",
    "            result_list_cols['route_id'].extend([route_pairs.route_id for x in route_nodeids])\n",
    "            result_list_cols['node_id'].extend(route_nodeids)\n",
    "\n",
    "            details['geometry']\n",
    "\n",
    "            new_line = shapely.geometry.LineString(details['geometry']['coordinates'])\n",
    "            all_results_summary['geometry'].append(new_line)\n",
    "\n",
    "\n",
    "    # 'results/all_route_node_ids'\n",
    "    \n",
    "    ic.enable()\n",
    "    all_results[verb] = pd.DataFrame(result_list_cols)\n",
    "    ic(verb, all_results[verb].head())\n",
    "    all_results[verb].to_csv(f'results/demo/{verb}.csv', index=False)\n",
    "\n",
    "    !docker stop {container_guid[0]}\n",
    "\n",
    "# all_results_summary_df = pd.DataFrame(all_results_summary)\n",
    "# all_results_summary_df.to_csv('results/demo/all_routes_carinal_pairs_summary.csv')\n",
    "all_results_summary_gdf = geopandas.GeoDataFrame(all_results_summary, geometry='geometry', crs=\"EPSG:4326\")\n",
    "# all_results_summary_df.to_csv('results/demo/all_routes_carinal_pairs_summary.csv')\n",
    "all_results_summary_gdf.to_file('results/demo/all_routes_carinal_pairs.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ic(type(route))\n",
    "# r_dict = json.loads(route)\n",
    "j_str = json.dumps(route)  # , 'results/demo/route.json')\n",
    "with open(\"results/demo/route.json\", \"w\") as fb:\n",
    "    fb.write(j_str)\n",
    "\n",
    "# route_nodes = all_street_nodes[all_street_nodes.id.isin(route_nodeids)]\n",
    "# ic(route_nodes.head())\n",
    "# new_line = shapely.geometry.LineString(route_nodes.geometry.to_list())\n",
    "# ic(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#\n",
    "# Filter out cases where the failed routes (ie where the routing engine failed to provide a route)\n",
    "# By inspection there are 27 routes which failed - either returning non-result, or returing a zero-length route\n",
    "# for all 27 cases it failed in some manner for driving, cycling and walking.\n",
    "# In each case the route started or ended at one of (a) a dual carrage way slip road or (b) somewhere within a private car park.\n",
    "#\n",
    "#################################\n",
    "\n",
    "all_results_summary_df = pd.read_csv(\n",
    "    \"results/all_routes_carinal_pairs_summary.csv\", index_col=0\n",
    ")\n",
    "ic(len(all_results_summary_df))\n",
    "all_results_reverse_summary_df = pd.read_csv(\n",
    "    \"results/all_routes_carinal_pairs_reverse_summary.csv\", index_col=0\n",
    ")\n",
    "all_results_summary_df = all_results_summary_df.append(all_results_reverse_summary_df)\n",
    "ic(len(all_results_summary_df))\n",
    "ic(all_results_summary_df[\"weight_name\"].unique())\n",
    "\n",
    "# Remove both directions for any routes that failed (even if they only failed in one direction)\n",
    "failed_routes = all_results_summary_df[\n",
    "    all_results_summary_df[\"weight_name\"].eq(\"NoRoute\")\n",
    "][\"route_id\"]\n",
    "ic(len(failed_routes))\n",
    "failed_routes = failed_routes.append(-failed_routes)\n",
    "ic(len(failed_routes))\n",
    "ic(failed_routes)\n",
    "all_results_summary_df = all_results_summary_df[\n",
    "    all_results_summary_df.route_id.isin(failed_routes) == False\n",
    "]\n",
    "ic(len(all_results_summary_df))\n",
    "\n",
    "filtered_results_summary_df = all_results_summary_df[\n",
    "    all_results_summary_df.distance > 0\n",
    "]\n",
    "ic(len(filtered_results_summary_df))\n",
    "ic(filtered_results_summary_df[\"weight_name\"].unique())\n",
    "all_results_summary_df.head(2)\n",
    "# non_reversible_routes = all_results_summary_df.route_id[all_results_summary_df.route_id.isin(-all_results_summary_df.route_id)]\n",
    "# # self_join = all_results_summary_df.merge(all_results_summary_df, right_on='')\n",
    "\n",
    "# ic(all_results_summary_df.route_id.head())\n",
    "# ic(-all_results_summary_df.route_id.head())\n",
    "\n",
    "# ic(all_results_summary_df.head())\n",
    "# ic(all_results_reverse_summary_df.head())\n",
    "# filtered_results_summary_df = all_results_summary_df[all_results_summary_df.distance > 0]\n",
    "# ic(filtered_results_summary_df['weight_name'].unique())\n",
    "# filtered_results_summary_df = all_results_summary_df[all_results_summary_df != all_results_summary_df.start_node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reverse routes to the route_list_gdf\n",
    "# route_list_gdf['direction'] = None\n",
    "ic(len(route_list_gdf))\n",
    "route_list_gdf.head()\n",
    "temp = route_list_gdf.copy(deep=True)\n",
    "route_list_gdf[\"direction\"] = \"forward\"\n",
    "temp[\"direction\"] = \"reverse\"\n",
    "temp[\"route_id\"] = -temp[\"route_id\"]\n",
    "ic(len(temp))\n",
    "\n",
    "route_list_gdf = route_list_gdf.append(temp)\n",
    "ic(len(route_list_gdf))\n",
    "# ic(route_list_gdf['direction'].unique())\n",
    "# ic(temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_results_summary_df.head()\n",
    "\n",
    "# Pivot the table to compare different transport type side-by-side\n",
    "piv1 = filtered_results_summary_df.pivot(\n",
    "    index=\"route_id\",\n",
    "    columns=\"verb\",\n",
    "    values=[\"distance\", \"duration\", \"weight_name\", \"weight\"],\n",
    ")\n",
    "# Rename the columns (flatten the tuple base columns names that pivot generates)\n",
    "piv1.columns = [f\"{s1}_{s2}\" for (s1, s2) in piv1.columns.tolist()]\n",
    "ic(piv1.columns)\n",
    "\n",
    "# Join the origional route planning data\n",
    "piv1[\"route_bi_id\"] = abs(piv1.index)\n",
    "ic(piv1.head(1))\n",
    "# left_on='route_bi_id', right_on='route_id')\n",
    "piv = piv1.merge(route_list_gdf, how=\"inner\", on=\"route_id\")\n",
    "piv = geopandas.GeoDataFrame(piv, geometry=\"geometry\", crs=crs_long_lat)\n",
    "piv = piv.to_crs(crs_bng)\n",
    "#'EPSG:27700'\n",
    "# Exclude any cases where start and end node are within a meter or two\n",
    "\n",
    "# Now calculate the ratios in length of the different transport types\n",
    "# Note: The \"s\" is \"s_length\" == the straight line distance. Added this to its own column for convenience\n",
    "piv[\"s_length\"] = piv.apply(lambda row: row[\"geometry\"].length, axis=1)\n",
    "piv[\"d_over_c_length\"] = (1.0 * piv.distance_driving / piv.distance_cycling).astype(\n",
    "    \"float64\"\n",
    ")\n",
    "piv[\"d_over_w_length\"] = (1.0 * piv.distance_driving / piv.distance_walking).astype(\n",
    "    \"float64\"\n",
    ")\n",
    "piv[\"c_over_w_length\"] = (1.0 * piv.distance_cycling / piv.distance_walking).astype(\n",
    "    \"float64\"\n",
    ")\n",
    "\n",
    "piv[\"d_over_s_length\"] = (1.0 * piv.distance_driving / piv.s_length).astype(\"float64\")\n",
    "piv[\"c_over_s_length\"] = (1.0 * piv.distance_cycling / piv.s_length).astype(\"float64\")\n",
    "piv[\"w_over_s_length\"] = (1.0 * piv.distance_walking / piv.s_length).astype(\"float64\")\n",
    "\n",
    "\n",
    "# piv[['d_over_c_length', 'd_over_w_length']].head()\n",
    "# piv_short = piv[['d_over_c_length', 'd_over_w_length']]\n",
    "# piv_short.info()\n",
    "# ic(piv.sort_values('d_over_c_length').head())\n",
    "# interesting_routes = piv[piv.d_over_c_length > 1.2]\n",
    "# ic(len(interesting_routes))\n",
    "# all_results_summary_df.head()\n",
    "# interesting_routes.head()\n",
    "# route_list_gdf.head(1)\n",
    "# piv.info\n",
    "# ic(max(piv.d_over_c_length))\n",
    "# ic(min(piv.d_over_c_length))\n",
    "# DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False,\n",
    "# right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)[source]\n",
    "\n",
    "# # route_list_gdf.head(1)\n",
    "# piv.head()\n",
    "# a = piv.columns\n",
    "# piv[:,([x == ('distance', 'driving') for x in a])]\n",
    "# b = piv.get(('distance', 'driving'))\n",
    "# a.head(1)\n",
    "# b = piv.loc[:,[('distance', 'driving')]].iloc[0]\n",
    "# type(b)\n",
    "# b.head(1)\n",
    "# pd.show_versions()\n",
    "piv_short = piv[\n",
    "    [\n",
    "        \"route_id\",\n",
    "        \"route_bi_id\",\n",
    "        \"s_length\",\n",
    "        \"d_over_c_length\",\n",
    "        \"d_over_w_length\",\n",
    "        \"c_over_w_length\",\n",
    "        \"d_over_s_length\",\n",
    "        \"c_over_s_length\",\n",
    "        \"w_over_s_length\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# piv_short = piv[[\n",
    "#     'route_id'\n",
    "# ]].copy()\n",
    "\n",
    "# ic(piv_short.describe())\n",
    "piv[[\"route_id\", \"route_bi_id\"]].hist()\n",
    "# route_list_gdf[['route_id']].hist()\n",
    "# route_list_gdf.head(2)\n",
    "# ic(piv.crs)\n",
    "\n",
    "\n",
    "# piv['d_over_c_length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there where exactly two routes (ie exclude cases where only one of the forward or reverse route was generated)\n",
    "piv_count = (\n",
    "    piv[[\"route_bi_id\", \"d_over_w_length\"]]\n",
    "    .groupby(\"route_bi_id\", as_index=False)\n",
    "    .count()\n",
    ")\n",
    "# ic(len(piv_count))\n",
    "ic(piv_count.head(2))\n",
    "failed_route_ids = piv_count[piv_count[\"d_over_w_length\"].eq(2) == False]\n",
    "ic(failed_route_ids.head(2))\n",
    "# failed_route_ids = failed_route_ids['route_bi_id']\n",
    "# ic(failed_route_ids)\n",
    "# ic(len(piv_short))\n",
    "# ic((piv_count[piv_count['d_over_w_length'].eq(2)==False]))\n",
    "piv = piv[piv[\"route_bi_id\"].isin(failed_route_ids[\"route_bi_id\"]) == False]\n",
    "# ic(len(piv_short), len(piv_short2), len(piv_short)-len(piv_short2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#\n",
    "# Find the min values of each ratio, between forward and reverse route direction\n",
    "\n",
    "# piv_bi = piv_short[['route_bi_id','d_over_w_length']].groupby('route_bi_id').idxmin(skipna=False)\n",
    "\n",
    "# a = piv_short[piv_short['route_bi_id'].eq(67544)]\n",
    "piv_bi = (\n",
    "    piv[[\"route_bi_id\", \"d_over_w_length\"]].groupby(\"route_bi_id\").idxmin(skipna=False)\n",
    ")\n",
    "piv_short3 = piv.loc[piv_bi[\"d_over_w_length\"].to_list()]\n",
    "ic(len(piv), len(piv_short3), len(piv_short) - len(piv_short3))\n",
    "\n",
    "# ic(len(piv_bi))\n",
    "# ic(piv_bi.columns)\n",
    "# ic(piv_bi.head(2))\n",
    "# ic(len(piv_bi[piv_bi['d_over_w_length'].isna()]))\n",
    "# ic(piv_short.index.dtype)\n",
    "# ic(piv_bi['d_over_w_length'].dtype)\n",
    "# piv_bi_dna = piv_bi['d_over_w_length'].dropna().astype('int32')\n",
    "# ic(piv_bi_dna.head(2))\n",
    "# ic(len(piv_bi_dna))\n",
    "# ic(len(piv_bi_dna[piv_bi_dna.isna()]))\n",
    "\n",
    "# piv_short3 = piv_short[:,piv_bi_dna['route_bi_id']]\n",
    "# ic(len(piv_short))\n",
    "# ic(len(piv_short3))\n",
    "\n",
    "# piv_short [:,piv_bi['d_over_w_length']\n",
    "\n",
    "# piv_short2 = piv_short[:,piv_bi['d_over_w_length'].dropna().astype('int32')]\n",
    "# ic(len(piv_short), len(piv_short2), len(piv_short)-len(piv_short2))\n",
    "\n",
    "# ic(piv_short2.head(2))\n",
    "\n",
    "# piv_bi2 = piv_short.groupby('route_bi_id').idxmax()\n",
    "# # ic(piv_bi2['d_over_w_length'].head(2))\n",
    "# ic(piv_bi2.head(2))\n",
    "# ic(piv.index)\n",
    "\n",
    "# Join back into route_list_gdf\n",
    "# piv_bi = piv_bi.merge(route_list_gdf, how='inner', on='route_id')\n",
    "# piv_bi.head()\n",
    "# piv_bi['route_id'].hist()\n",
    "# piv_short.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_short3.head(2)\n",
    "quan_90 = (\n",
    "    piv_short3[[\"intented_offset_dist\", \"d_over_w_length\"]]\n",
    "    .groupby(\"intented_offset_dist\")\n",
    "    .quantile(0.9)\n",
    ")\n",
    "quan_90.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(piv_short.describe())\n",
    "# ic(piv_short.head())\n",
    "\n",
    "# diverge_routes = piv[piv.d_over_w_length > 2.087483]\n",
    "# diverge_routes_50_ids = diverge_routes[diverge_routes.intented_offset_dist == 50]\n",
    "# diverge_routes_250_ids = diverge_routes[diverge_routes.intented_offset_dist == 250]\n",
    "# diverge_routes_1500_ids = diverge_routes[diverge_routes.intented_offset_dist == 1500]\n",
    "\n",
    "\n",
    "diverge_routes_50_ids = piv[\n",
    "    (piv.d_over_w_length > 1.348924) & (piv.intented_offset_dist == 50)\n",
    "]\n",
    "diverge_routes_250_ids = piv[\n",
    "    (piv.d_over_w_length > 2.886017) & (piv.intented_offset_dist == 250)\n",
    "]\n",
    "diverge_routes_1500_ids = piv[\n",
    "    (piv.d_over_w_length > 1.862107) & (piv.intented_offset_dist == 1500)\n",
    "]\n",
    "\n",
    "# for div_routes in [diverge_routes_50_ids, diverge_routes_250_ids, diverge_routes_1500_ids]:\n",
    "#     ic(div_routes[['route_id', 'd_over_w_length']].describe())\n",
    "\n",
    "ic(diverge_routes_50_ids.head())\n",
    "\n",
    "walking_all_nodes = pd.read_csv(f\"results/all_route_list_carinal_pairs_ids/walking.csv\")\n",
    "\n",
    "hw50 = walking_all_nodes[\n",
    "    walking_all_nodes[\"route_id\"].isin(diverge_routes_50_ids.route_id)\n",
    "]\n",
    "hw50 = hw50.groupby(\"node_id\", as_index=True).count()\n",
    "hw50_gdf = all_street_nodes.merge(\n",
    "    hw50, how=\"inner\", left_on=\"id\", right_on=\"node_id\", copy=True\n",
    ")\n",
    "hw50_gdf.to_file(\"results/hw50.shp\")\n",
    "\n",
    "hw250 = walking_all_nodes[\n",
    "    walking_all_nodes[\"route_id\"].isin(diverge_routes_250_ids.route_id)\n",
    "]\n",
    "hw250 = hw250.groupby(\"node_id\", as_index=True).count()\n",
    "hw250_gdf = all_street_nodes.merge(\n",
    "    hw250, how=\"inner\", left_on=\"id\", right_on=\"node_id\", copy=True\n",
    ")\n",
    "hw250_gdf.to_file(\"results/hw250.shp\")\n",
    "\n",
    "hw1500 = walking_all_nodes[\n",
    "    walking_all_nodes[\"route_id\"].isin(diverge_routes_1500_ids.route_id)\n",
    "]\n",
    "hw1500 = hw1500.groupby(\"node_id\", as_index=True).count()\n",
    "hw1500_gdf = all_street_nodes.merge(\n",
    "    hw1500, how=\"inner\", left_on=\"id\", right_on=\"node_id\", copy=True\n",
    ")\n",
    "hw1500_gdf.to_file(\"results/hw1500.shp\")\n",
    "\n",
    "# highlighted_walking_nodes = hw50.join(hw250, lsuffix='_50', rsuffix='_250').join(hw1500, rsuffix='_1500')\n",
    "\n",
    "# ic(len(highlighted_walking_nodes))\n",
    "\n",
    "\n",
    "# ic(len(highlighted_walking_nodes))\n",
    "# ic(highlighted_walking_nodes.columns)\n",
    "# ic(all_street_nodes.head())\n",
    "# # highlighted_walking_gdf = all_street_nodes[ all_street_nodes['id'].isin(highlighted_walking_nodes['node_id']) ]\n",
    "\n",
    "# highlighted_walking_gdf = all_street_nodes.merge(highlighted_walking_nodes, how='inner', left_on='id', right_on='node_id', copy=True)\n",
    "\n",
    "# highlighted_walking_gdf.to_file('results/highlighted_walking.shp')\n",
    "# # single_gdf = all_street_nodes.join(single_gdf, how='inner', lsuffix='all', rsuffix=verb)\n",
    "\n",
    "\n",
    "# b = ['a','b','c']\n",
    "# # filter out secound geometry column before exporting\n",
    "# a = a[[col for col in a.columns if col != 'geometryall']]\n",
    "# # ic(len(a))\n",
    "# a.head(1)\n",
    "\n",
    "# a.to_file('results/d_over_w_length_90.shp')\n",
    "# # piv_short['d_over_c_length'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link back to address data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_street_node_gdf = geopandas.read_file(\"results/address_street_node_pairs.shp\")\n",
    "address_street_node_gdf.columns = [\"UPRN\", \"street_node\", \"geometry\"]\n",
    "address_polys_gdf = geopandas.read_file(\"osopenuprn_202108_csv/address_polys.shp\")\n",
    "# hw50_gdf\n",
    "# hw250_gdf\n",
    "lookup_df = address_street_node_gdf[[\"UPRN\", \"street_node\"]].copy()\n",
    "\n",
    "# ic(hw1500_gdf.head(2))\n",
    "# ic(lookup_df.head(2))\n",
    "# ic(aoi_uprn_df.head(2))\n",
    "# ic(address_polys_gdf.head(2))\n",
    "# ic(address_street_node_gdf.head(2))\n",
    "\n",
    "hw_gdfs = {\"50m\": hw50_gdf, \"250m\": hw250_gdf, \"1500m\": hw1500_gdf}\n",
    "\n",
    "for label, hw_gdf in hw_gdfs.items():\n",
    "    temp = lookup_df.merge(\n",
    "        hw_gdf, left_on=\"street_node\", right_on=\"id\", how=\"inner\", copy=True\n",
    "    )\n",
    "    ltn_points_df = aoi_uprn_df.merge(\n",
    "        temp, on=\"UPRN\", how=\"inner\", copy=True, suffixes=(\"\", \"_DROP_ME\")\n",
    "    )\n",
    "    # ic(ltn_points_df.head(2))\n",
    "    ltn_points_df = ltn_points_df[\n",
    "        [col for col in ltn_points_df.columns if not \"_DROP_ME\" in col]\n",
    "    ]\n",
    "    # ic(ltn_points_df.head(2))\n",
    "    ltn_points_gdf = geopandas.GeoDataFrame(\n",
    "        ltn_points_df, geometry=\"geometry\", crs=aoi_uprn_df.crs\n",
    "    )\n",
    "    ltn_points_gdf.to_file(f\"results/derived_ltn_point_{label}.shp\")\n",
    "\n",
    "    ltn_polys_df = address_polys_gdf.merge(\n",
    "        temp, on=\"UPRN\", how=\"inner\", copy=True, suffixes=(\"\", \"_DROP_ME\")\n",
    "    )\n",
    "    ltn_polys_df = ltn_polys_df[\n",
    "        [col for col in ltn_polys_df.columns if not \"_DROP_ME\" in col]\n",
    "    ]\n",
    "    ltn_polys_gdf = geopandas.GeoDataFrame(\n",
    "        ltn_polys_df, geometry=\"geometry\", crs=address_polys_gdf.crs\n",
    "    )\n",
    "    ltn_polys_gdf.to_file(f\"results/derived_ltn_poly_{label}.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate difference between different travel styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(all_street_nodes.head())\n",
    "ic.disable()\n",
    "single_df = None\n",
    "\n",
    "all_results = {}\n",
    "for verb, values in osrm_docker_params.items():\n",
    "    all_results[verb] = pd.read_csv(\n",
    "        f\"results/all_route_list_carinal_pairs_ids/{verb}.csv\"\n",
    "    )\n",
    "    # all_results[verb].head()\n",
    "\n",
    "for verb, df in all_results.items():\n",
    "    ic(verb)\n",
    "    count_df = df.groupby(\"node_id\", as_index=False).count()\n",
    "    ic(count_df)\n",
    "    ic(count_df.columns)\n",
    "    # ic(all_street_nodes.columns)\n",
    "    # count_df.rename(columns={\"count_df.head()\":\"count_of_routes\"}, inplace=True)\n",
    "    # ic(count_df.head())\n",
    "    # route_node_df =  all_street_nodes[all_street_nodes.id.isin(route_nodeids)]\n",
    "    # single_gdf = all_street_nodes.join(count_df, how='inner')\n",
    "\n",
    "    # node_count_gdf = all_street_nodes.join(count_df.set_index('node_id'))\n",
    "    # count_df.rename({lambda x: x+verb}, axis='columns', inplace=True)\n",
    "    count_df.rename(columns={\"route_id\": f\"{verb}_count\"}, inplace=True)\n",
    "    ic(count_df.columns)\n",
    "    if single_df is None:\n",
    "        single_df = count_df.set_index(\"node_id\")\n",
    "    else:\n",
    "        single_df = single_df.join(\n",
    "            count_df.set_index(\"node_id\"), how=\"outer\", rsuffix=verb\n",
    "        )\n",
    "\n",
    "    # ic(len(single_df))\n",
    "    # ic(single_df.columns)\n",
    "\n",
    "# single_gdf = all_street_nodes.join(single_gdf, how='inner', lsuffix='all', rsuffix=verb)\n",
    "\n",
    "single_df = single_df.fillna(value=0)\n",
    "\n",
    "single_df[\"d_c\"] = single_df.apply(\n",
    "    lambda row: row[\"driving_count\"] - row[\"cycling_count\"], axis=1\n",
    ")\n",
    "single_df[\"d_w\"] = single_df.apply(\n",
    "    lambda row: row[\"driving_count\"] - row[\"walking_count\"], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "ic(single_df.head())\n",
    "# single_gdf.plot(column='route_id')\n",
    "\n",
    "single_gdf = all_street_nodes.set_index(\"id\").join(single_df, how=\"inner\")\n",
    "ic(single_gdf.head())\n",
    "single_gdf.plot(column=\"d_c\")\n",
    "\n",
    "single_gdf.to_file(\"results/count_by_mode_carinal_pairs.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = all_results[\"cycling\"]\n",
    "# cdf.head()\n",
    "interesting_route_ids = interesting_routes.index.values\n",
    "# # interesting_routes.\n",
    "interesting_nodes = cdf.loc[interesting_route_ids].set_index(\"node_id\")\n",
    "ic(interesting_nodes.head())\n",
    "ic(len(cdf))\n",
    "ic(len(interesting_nodes))\n",
    "ic(len(interesting_route_ids))\n",
    "\n",
    "interesting_count_df = interesting_nodes.groupby(\"node_id\", as_index=True).count()\n",
    "interesting_count_df.rename(columns={\"route_id\": \"cycling_count\"}, inplace=True)\n",
    "ic(len(interesting_count_df))\n",
    "ic(interesting_count_df.columns)\n",
    "interesting_count_df.info()\n",
    "interesting_count_df = interesting_count_df.fillna(value=0)\n",
    "ic.enable()\n",
    "ic(interesting_count_df.head())\n",
    "ic(all_street_nodes.head())\n",
    "\n",
    "interesting_gdf = all_street_nodes.join(interesting_count_df, how=\"inner\", on=\"id\")\n",
    "ic(interesting_gdf.head())\n",
    "interesting_gdf.plot(column=\"cycling_count\")\n",
    "interesting_gdf.to_file(\"results/interesting_nodes_carinal_pairs.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull it all together in a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(\n",
    "    basemap=basemaps.OpenStreetMap.Mapnik,\n",
    "    center=(55.00713, -1.6202478),  # 55.0042099, -1.6202478\n",
    "    zoom=16,\n",
    ")\n",
    "# ic(m)\n",
    "\n",
    "# hover_style={'fillColor': 'red' , 'fillOpacity': 0.2},\n",
    "\n",
    "wards_geo_data = GeoData(\n",
    "    geo_dataframe=aoi_wards_ll,\n",
    "    style={\n",
    "        \"color\": \"black\",\n",
    "        \"fillColor\": \"darkblue\",\n",
    "        \"opacity\": 0.5,\n",
    "        \"weight\": 1.9,\n",
    "        \"dashArray\": \"2\",\n",
    "        \"fillOpacity\": 0,\n",
    "    },\n",
    "    name=\"Wards\",\n",
    ")\n",
    "# ic(geo_data)\n",
    "\n",
    "uprn_geo_data = GeoData(\n",
    "    geo_dataframe=aoi_uprn_df,\n",
    "    style={\"color\": \"red\", \"opacity\": 0.5, \"radius\": 1},\n",
    "    point_style={\"color\": \"red\", \"opacity\": 0.5, \"radius\": 1},\n",
    "    name=\"UPRN\",\n",
    ")\n",
    "\n",
    "street_nodes = GeoData(\n",
    "    geo_dataframe=all_street_nodes,\n",
    "    style={\"color\": \"blue\", \"opacity\": 0.5, \"radius\": 1},\n",
    "    point_style={\"color\": \"blue\", \"opacity\": 0.5, \"radius\": 1},\n",
    "    name=\"Street Nodes\",\n",
    ")\n",
    "\n",
    "test_route = GeoData(\n",
    "    geo_dataframe=route_node_df,\n",
    "    style={\n",
    "        \"color\": \"green\",\n",
    "        \"fillColor\": \"green\",\n",
    "        \"opacity\": 1,\n",
    "        \"fillOpacity\": 1,\n",
    "        \"radius\": 3,\n",
    "    },\n",
    "    point_style={\"color\": \"green\", \"opacity\": 1, \"radius\": 3},\n",
    "    name=\"test_route\",\n",
    ")\n",
    "\n",
    "results = GeoData(\n",
    "    geo_dataframe=single_gdf, choro_data=single_gdf, key_on=\"d_c\", name=\"results\"\n",
    ")\n",
    "\n",
    "\n",
    "single_gdf\n",
    "\n",
    "\n",
    "m.add_layer(wards_geo_data)\n",
    "# m.add_layer(street_nodes)\n",
    "# m.add_layer(uprn_geo_data)\n",
    "m.add_layer(test_route)\n",
    "# m.add_layer(results)\n",
    "m.add_control(LayersControl())\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gosforth-roads.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "4c8f17387fbe8c7287bbe19cc3cccd9390a6f55a076ebb6fd4de774d1e88578d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('env39': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
